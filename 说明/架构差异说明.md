# SAETR实现与理论架构的差异说明

## 1. 稀疏正则化实现方式
### 理论要求
- 仅在编码层输出后应用稀疏惩罚（公式14）

### 实际实现
```python
# model.py SAE.forward
sparse_loss = torch.mean(torch.abs(z))  # L1正则化
```
- 将稀疏损失作为独立项返回
- 在训练时通过`beta_sparse`系数控制权重（见train.py）

## 2. CNN特征增强模块
### 理论架构
- 未提及该设计

### 实际实现
```python
# model.py SAETR.__init__
if use_cnn:
    self.cnn = nn.Sequential(
        nn.Conv1d(...),
        nn.ReLU(),
        nn.Conv1d(...),
        nn.ReLU()
    )
```
- 在SAE编码和Transformer之间加入1D CNN层
- 增强局部特征提取能力
- 可通过配置开关（默认启用）

## 3. 损失函数组合
### 理论公式
总损失 = 预测损失 + 重建损失

### 实际实现
```python
# train.py train_epoch
loss = loss_main + alpha_recon * loss_rec + beta_sparse * loss_sp
```
- 添加第三项稀疏正则损失
- 超参数`alpha_recon`和`beta_sparse`独立控制权重

## 4. 改进效果
1. 增强特征稀疏性：显式L1正则提升特征选择能力
2. 局部特征增强：CNN模块捕获时序局部模式
3. 灵活的正则控制：独立系数调整不同损失项的贡献